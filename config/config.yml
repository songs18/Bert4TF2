data_config:
    input_file: ./files/sample_text.txt
    output_file: ./files/train_examples.tfrecord
    vocab_file: ./files/vocab.txt
    do_lower_case: True
    do_whole_word_mask: False
    max_seq_length: 128
    max_predictions_per_seq: 20
    masked_lm_prob: 0.15
    random_seed: 12345
    dupe_factor: 5
    short_seq_prob: 0.1


model_config:
    name: bert
    do_train: True
    do_eval: True

#    hidden_size: 128
#    embedding_size: 128
#    hidden_act: gelu
#    initializer_range: 0.02
#    vocab_size: 30522
#    hidden_dropout_prob: 0.1
#    num_attention_heads: 2
#    type_vocab_size: 2
#    max_position_embeddings: 512
#    num_hidden_layers: 2
#    intermediate_size: 512
#    attention_probs_dropout_prob: 0.1
#    hidden_dropout_prob: 0.1
#    initializer_range: 0.02

    embedding_layer_config:
        name: embeddings
        use_token_type: True
        use_position_embeddings: True
        dropout_prob: 0.1

        word_piece_embedding_layer_config:
            name: word_embeddings
            vocab_size: 30522
            embedding_size: 128
            initializer_range: 0.02
            use_one_hot_embeddings: False

        token_type_embedding_layer_config:
            name: token_type_embeddings
            vocab_size: 2
            width:
            initializer_range: 0.02

        position_embedding_layer_config:
            name: position_embeddings
            max_position_embeddings: 512
            width:
            initializer_range: 0.02
#    处理激活函数
    transformer_layers_config:
        name: encoder
        num_hidden_layers: 2
        do_return_all_layers: True

        single_layer_config:
            selfattention_sublayer_config:
                name: attention
                self:
                    name: self
                    hidden_size: 128
                    act_str: gelu
                    num_attention_heads: 2
                    initializer_range: 0.02
                    hidden_dropout_prob: 0.1
                    attention_probs_dropout_prob: 0.1
                    do_return_2d_tensor: True

                output:
                    name: output
                    initializer_range: 0.02
                    hidden_size: 128
                    hidden_dropout_prob: 0.1

            feedforward_sublayer_config:
                name: intermediate

                intermediate:
                    name: intermediate
                    intermediate_size: 512
                    initializer_range: 0.02
                    act_str: gelu
                output:
                    name: output
                    hidden_size: 128
                    initializer_range: 0.02
                    hidden_dropout_prob: 0.1

    pooling_layer_config:
        name: pooler
        initializer_range: 0.02
        hidden_size: 128
        act_str: gelu

    word_cls_layer_config:
        name: cls/predictions
        initializer_range: 0.02
        hidden_size: 128
        act_str: gelu
        vocab_size: 30522

    next_sentence_layer_config:
        name: cls/seq_relationship
        initializer_range: 0.02
        hidden_size: 128


running_config:
    IO_config:
        #  Input TF example files (can be a glob or comma separated).
        input_files: ./files/train_examples.tfrecord
        #  The output directory where the nn checkpoints will be written.
        output_dir: ./files/eval/
        model_dir: ./files/checkpoints/train
        #  Initial checkpoint (usually from a pre-trained BERT nn).
        init_checkpoint: ./files/checkpoints/tiny/bert_model.ckpt

    device_config:
        use_tpu: False
        tpu_name: None
        tpu_zone: None
        gcp_project: None
        master: None
        num_tpu_cores: None

    training_config:
        #  The maximum total input sequence length after WordPiece tokenization.
        max_seq_length: 128
        #  Maximum number of masked LM predictions per sequence. Must match data generation.
        #  Sequences longer than this will be truncated, and sequences shorter than this will be padded. Must match data generation.
        max_predictions_per_seq: 20

        batch_size: 32
        random_seed: 12345

        learning_rate: 5e-5
        num_train_steps: 100000
        num_warmup_steps: 10000

        save_checkpoints_steps: 1000
        iterations_per_loop: 1000

    eval_config:
        batch_size: 8
        max_eval_steps: 100